# RAG-AI-AGENT

A **Retrieval-Augmented Generation (RAG)** web app that answers questions **strictly from your PDFs** using **FAISS** vector search, **Hugging Face embeddings**, and an **LLM hosted on Hugging Face Inference**. It includes a clean UI, friendly â€œnot foundâ€ messages with emojis, and clickable links in answers.

***

## âœ¨ Features

*   ğŸ” **Multiâ€‘PDF indexing**: Seamlessly combine multiple PDFs into one FAISS index
*   ğŸ§  **Semantic search** with `sentence-transformers/allâ€‘MiniLMâ€‘L6â€‘v2`
*   ğŸ¤– **Grounded answers**: LLM uses only retrieved context from your PDFs
*   âœ… **Clear UX**:
    *   â€œ**Answer Found**â€ card only when a real answer exists
    *   Friendly **â€œSorry ğŸ˜”â€¦ not in the PDFâ€** panel when content isnâ€™t found
*   ğŸ”— **Clickable links** in responses
*   ğŸ” **`.env` support** for secure tokens
*   ğŸ **Virtual environment** friendly (Windows/macOS/Linux)

***

## ğŸ“ Project Structure

    rag-ai-agent/
    â”‚
    â”œâ”€â”€ app.py                 # Streamlit app (RAG UI + pipeline)
    â”œâ”€â”€ requirements.txt       # Python dependencies
    â”œâ”€â”€ .env                   # Holds HF_TOKEN (not committed)
    â”œâ”€â”€ qa_pdf/
    â”‚   â”œâ”€â”€ rag_sample_qa.pdf  # 20 Q&A sample PDF
    â”‚   â””â”€â”€ rag_50_questions.pdf  # 50 Q&A support-style PDF
    â””â”€â”€ README.md

> You can add more PDFs inside `qa_pdf/` and update the list in the app to include them.

***

## ğŸ” Environment Variables

Create a `.env` file at the project root:

```env
HF_TOKEN=hf_your_real_token_here
```

*   Get your token: <https://huggingface.co/settings/tokens>
*   **No quotes**, **no spaces**, just `KEY=VALUE`.

***

## ğŸ Set Up a Virtual Environment

> **Python 3.10+** recommended (3.11 preferred)

### Windows (PowerShell)

```powershell
cd rag-ai-agent
py -3.11 -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
```

> If `py` is not available, use:
>
> ```powershell
> python -m venv .venv
> .\.venv\Scripts\Activate.ps1
> ```

### macOS / Linux

```bash
cd rag-ai-agent
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
```

***

## ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

**`requirements.txt`**

    streamlit
    langchain
    langchain-community
    langchain-huggingface
    langchain-core
    sentence-transformers
    faiss-cpu
    pypdf
    python-dotenv
    transformers

***

## ğŸ“š Add/Verify PDFs

This app ships with two sample PDFs for testing:

*   `qa_pdf/rag_sample_qa.pdf` (20 Q\&A)
*   `qa_pdf/rag_50_questions.pdf` (50 Q\&A)

You can add more PDFs to `qa_pdf/`, and then include their paths in the list used by the app.

***

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

Open the local URL shown in the terminal (typically <http://localhost:8501>).  
Type your question in the text area and click **Submit**.

*   If a relevant answer is found in your PDFs â†’ âœ… **Answer Found** (green card)
*   If not â†’ ğŸ˜” **Sorry** message with suggestions to **try another question**

***

## ğŸ§  How It Works

1.  **Load PDFs** using `PyPDFLoader`
2.  **Split** into chunks with `RecursiveCharacterTextSplitter`
3.  **Embed** chunks using `HuggingFaceEmbeddings` (`allâ€‘MiniLMâ€‘L6â€‘v2`)
4.  **Index** with FAISS (single combined index across all PDFs)
5.  **Retrieve** topâ€‘K chunks for a query
6.  **Generate** a grounded answer via a **Hugging Face Inference** instruct model
7.  **Polish** the UI: success/error panels, emojis, and **clickable links**

***

## ğŸ¤– Model Notes

The app uses a Hugging Face Inference LLM. If your `HF_TOKEN` doesnâ€™t have access to the default model, switch to a model you can use by changing the `repo_id` in `app.py`, e.g.:

*   `mistralai/Mistral-7B-Instruct-v0.2`
*   `meta-llama/Meta-Llama-3-8B-Instruct`

> You can browse models at <https://huggingface.co/models> (filter for **instruct** or **chat** variants).

***

## ğŸ§ª Troubleshooting

*   **`HF_TOKEN missing`**  
    Ensure `.env` exists and contains:  
    `HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXX`  
    Then restart the app.

*   **Model access denied (403/404)**  
    Your token may not have access to the default model. Choose a public instruct model and update `repo_id`.

*   **No answers for known questions**
    *   Check the PDF actually contains the info
    *   Increase `TOP_K` (e.g., 6)
    *   Tune chunking: `chunk_size=600`, `chunk_overlap=80`

*   **ModuleNotFoundError**  
    Make sure your virtual environment is **active** and you ran `pip install -r requirements.txt`.

*   **PowerShell script execution policy (Windows)**  
    If activation fails:  
    `Set-ExecutionPolicy Unrestricted -Scope CurrentUser`  
    Then re-run: `.\.venv\Scripts\Activate.ps1`

***

## â• Extending (Ideas)

*   ğŸ“ **PDF Uploader**: Ingest any PDF at runtime
*   ğŸ’¾ **Persistent FAISS**: Save/load index to speed up startup
*   ğŸ’¬ **Chat history + streaming responses**
*   ğŸ¨ **Branding / Theme toggle / Sidebar settings**
*   ğŸ³ **Docker**: Containerize for deployment

***

## ğŸ¤ Contributing

1.  Fork the repository
2.  Create a feature branch: `git checkout -b feat/your-feature`
3.  Commit your changes: `git commit -m "Add your feature"`
4.  Push to your branch: `git push origin feat/your-feature`
5.  Open a Pull Request

***

## ğŸ“ License

This project is released under the **MIT License**.  
Feel free to use, modify, and distribute.

***

## ğŸ™ Acknowledgments

*   Streamlit
*   LangChain
*   Hugging Face
*   Sentence Transformers
*   FAISS

***
